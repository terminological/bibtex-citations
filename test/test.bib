@article{Amodei,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing atten-tion to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (" avoiding side effects " and " avoiding reward hacking "), an objective function that is too expensive to evaluate frequently (" scalable supervision "), or undesirable behavior during the learning process (" safe exploration " and " distributional shift "). We review previous work in these areas as well as suggesting re-search directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Openai, John Schulman and Man{\'{e}}, Dan and Brain, Google},
file = {::},
title = {{Concrete Problems in AI Safety}},
url = {https://arxiv.org/pdf/1606.06565.pdf}
}
@article{Blois1980,
abstract = {Abstract The increasingly frequent application of formal methods, including algorithms and computer programs, to processes that are ordinarily viewed as judgmental seems to be a source of both promise and unease for physicians. A consideration of some of these methods suggests that it may be helpful to attempt to distinguish carefully between judgment and computation. Medical care involves a complex of inferential processes, any of which may be performed as judgments, and some of which may be carried out as a computation. My purpose here is to identify the latter cases. The empirical evidence suggests that such a demarcation is feasible. The most important question appears not to be "Where can we use computers?" but "Where must we use human beings?" Until this matter is more thoroughly explored, tension between physicians and computer advocates will persist. (N Engl J Med. 1980; 303:192–7.)},
author = {Blois, Marsden S.},
doi = {10.1056/NEJM198007243030405},
issn = {0028-4793},
journal = {New England Journal of Medicine},
month = {jul},
number = {4},
pages = {192--197},
publisher = { Massachusetts Medical Society },
title = {{Clinical Judgment and Computers}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJM198007243030405},
volume = {303},
year = {1980}
}
@misc{KaneSPCliniCalc,
author = {{Kane SP (CliniCalc)}},
title = {{CHADS2 Calculator for Guiding Antithrombotic Treatment in Atrial Fibrillation}},
url = {http://clincalc.com/cardiology/stroke/CHADS.aspx},
urldate = {2017-05-15}
}
@article{Gulshan2016,
abstract = {Importance Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation. Objective To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs. Design and Setting A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency. Exposure Deep learning-trained algorithm. Main Outcomes and Measures The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity. Results The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2{\%} women; prevalence of RDR, 683/8878 fully gradable images [7.8{\%}]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6{\%} women; prevalence of RDR, 254/1745 fully gradable images [14.6{\%}]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95{\%} CI, 0.988-0.993) for EyePACS-1 and 0.990 (95{\%} CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3{\%} (95{\%} CI, 87.5{\%}-92.7{\%}) and the specificity was 98.1{\%} (95{\%} CI, 97.8{\%}-98.5{\%}). For Messidor-2, the sensitivity was 87.0{\%} (95{\%} CI, 81.1{\%}-91.0{\%}) and the specificity was 98.5{\%} (95{\%} CI, 97.7{\%}-99.1{\%}). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5{\%} and specificity was 93.4{\%} and for Messidor-2 the sensitivity was 96.1{\%} and specificity was 93.9{\%}. Conclusions and Relevance In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.},
author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
doi = {10.1001/jama.2016.17216},
file = {:home/rc538/Documents/References/JAMA/Gulshan et al. - 2016 - Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus.pdf:pdf},
isbn = {1538-3598 (Electronic) 0098-7484 (Linking)},
issn = {0098-7484},
journal = {JAMA},
keywords = {diabetic,diabetic retinopathy,fundus photography,machine learning,macular edema,neural networks (computer)},
month = {dec},
number = {22},
pages = {2402},
pmid = {27898976},
title = {{Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27898976 http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2016.17216},
volume = {316},
year = {2016}
}
@article{Afzal2017,
abstract = {OBJECTIVE Lower extremity peripheral arterial disease (PAD) is highly prevalent and affects millions of individuals worldwide. We developed a natural language processing (NLP) system for automated ascertainment of PAD cases from clinical narrative notes and compared the performance of the NLP algorithm with billing code algorithms, using ankle-brachial index test results as the gold standard. METHODS We compared the performance of the NLP algorithm to (1) results of gold standard ankle-brachial index; (2) previously validated algorithms based on relevant International Classification of Diseases, Ninth Revision diagnostic codes (simple model); and (3) a combination of International Classification of Diseases, Ninth Revision codes with procedural codes (full model). A dataset of 1569 patients with PAD and controls was randomly divided into training (n = 935) and testing (n = 634) subsets. RESULTS We iteratively refined the NLP algorithm in the training set including narrative note sections, note types, and service types, to maximize its accuracy. In the testing dataset, when compared with both simple and full models, the NLP algorithm had better accuracy (NLP, 91.8{\%}; full model, 81.8{\%}; simple model, 83{\%}; P {\textless} .001), positive predictive value (NLP, 92.9{\%}; full model, 74.3{\%}; simple model, 79.9{\%}; P {\textless} .001), and specificity (NLP, 92.5{\%}; full model, 64.2{\%}; simple model, 75.9{\%}; P {\textless} .001). CONCLUSIONS A knowledge-driven NLP algorithm for automatic ascertainment of PAD cases from clinical notes had greater accuracy than billing code algorithms. Our findings highlight the potential of NLP tools for rapid and efficient ascertainment of PAD cases from electronic health records to facilitate clinical investigation and eventually improve care by clinical decision support.},
author = {Afzal, Naveed and Sohn, Sunghwan and Abram, Sara and Scott, Christopher G. and Chaudhry, Rajeev and Liu, Hongfang and Kullo, Iftikhar J. and Arruda-Olson, Adelaide M.},
doi = {10.1016/j.jvs.2016.11.031},
file = {:home/rc538/Documents/References/Journal of vascular surgery/Afzal et al. - 2017 - Mining peripheral arterial disease cases from narrative clinical notes using natural language processing.pdf:pdf},
issn = {1097-6809},
journal = {Journal of vascular surgery},
month = {feb},
pages = {1--9},
pmid = {28189359},
publisher = {The Authors},
title = {{Mining peripheral arterial disease cases from narrative clinical notes using natural language processing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28189359 http://linkinghub.elsevier.com/retrieve/pii/S0741521416318444 http://linkinghub.elsevier.com/retrieve/pii/S0741521416318444{\%}0Ahttp://www.ncbi.nlm.nih.gov/pubmed/28189359},
year = {2017}
}
@misc{Evans,
abstract = {From smartphone assistants to image recognition and translation, machine learning already helps us in our everyday lives. But it can also help us to tackle some of the world's most challenging physical problems -- such as energy consumption. Large-scale commercial and industrial systems like data centres consume a lot of energy, and while much has been done to stem the growth of energy use, there remains a lot more to do given the world's increasing need for computing power. Reducing energy usage has been a major focus for us over the past 10 years: we have built our own super-efficient servers at Google, invented more efficient ways to cool our data centres and invested heavily in green energy sources, with the goal of being powered 100 percent by renewable energy. Compared to five years ago, we now get around 3.5 times the computing power out of the same amount of energy, and we continue to make many improvements each year.},
author = {Evans, Richard (DeepMind) and Gao, Jim (DeepMind)},
title = {{DeepMind AI Reduces Google Data Centre Cooling Bill by 40{\%}}},
url = {https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/},
urldate = {2017-05-12}
}
@article{Hippisley-Cox2008,
abstract = {OBJECTIVE: To develop and validate version two of the QRISK cardiovascular disease risk algorithm (QRISK2) to provide accurate estimates of cardiovascular risk in patients from different ethnic groups in England and Wales and to compare its performance with the modified version of Framingham score recommended by the National Institute for Health and Clinical Excellence (NICE). DESIGN: Prospective open cohort study with routinely collected data from general practice, 1 January 1993 to 31 March 2008. SETTING: 531 practices in England and Wales contributing to the national QRESEARCH database. PARTICIPANTS: 2.3 million patients aged 35-74 (over 16 million person years) with 140,000 cardiovascular events. Overall population (derivation and validation cohorts) comprised 2.22 million people who were white or whose ethnic group was not recorded, 22,013 south Asian, 11,595 black African, 10,402 black Caribbean, and 19,792 from Chinese or other Asian or other ethnic groups. MAIN OUTCOME MEASURES: First (incident) diagnosis of cardiovascular disease (coronary heart disease, stroke, and transient ischaemic attack) recorded in general practice records or linked Office for National Statistics death certificates. Risk factors included self assigned ethnicity, age, sex, smoking status, systolic blood pressure, ratio of total serum cholesterol:high density lipoprotein cholesterol, body mass index, family history of coronary heart disease in first degree relative under 60 years, Townsend deprivation score, treated hypertension, type 2 diabetes, renal disease, atrial fibrillation, and rheumatoid arthritis. RESULTS: The validation statistics indicated that QRISK2 had improved discrimination and calibration compared with the modified Framingham score. The QRISK2 algorithm explained 43{\%} of the variation in women and 38{\%} in men compared with 39{\%} and 35{\%}, respectively, by the modified Framingham score. Of the 112,156 patients classified as high risk (that is, {\textgreater}or=20{\%} risk over 10 years) by the modified Framingham score, 46,094 (41.1{\%}) would be reclassified at low risk with QRISK2. The 10 year observed risk among these reclassified patients was 16.6{\%} (95{\%} confidence interval 16.1{\%} to 17.0{\%})-that is, below the 20{\%} treatment threshold. Of the 78 024 patients classified at high risk on QRISK2, 11,962 (15.3{\%}) would be reclassified at low risk by the modified Framingham score. The 10 year observed risk among these patients was 23.3{\%} (22.2{\%} to 24.4{\%})-that is, above the 20{\%} threshold. In the validation cohort, the annual incidence rate of cardiovascular events among those with a QRISK2 score of {\textgreater}or=20{\%} was 30.6 per 1000 person years (29.8 to 31.5) for women and 32.5 per 1000 person years (31.9 to 33.1) for men. The corresponding figures for the modified Framingham equation were 25.7 per 1000 person years (25.0 to 26.3) for women and 26.4 (26.0 to 26.8) for men). At the 20{\%} threshold, the population identified by QRISK2 was at higher risk of a CV event than the population identified by the Framingham score. CONCLUSIONS: Incorporating ethnicity, deprivation, and other clinical conditions into the QRISK2 algorithm for risk of cardiovascular disease improves the accuracy of identification of those at high risk in a nationally representative population. At the 20{\%} threshold, QRISK2 is likely to be a more efficient and equitable tool for treatment decisions for the primary prevention of cardiovascular disease. As the validation was performed in a similar population to the population from which the algorithm was derived, it potentially has a "home advantage." Further validation in other populations is therefore advised.},
author = {Hippisley-Cox, Julia and Coupland, Carol and Vinogradova, Yana and Robson, John and Minhas, Rubin and Sheikh, Aziz and Brindle, Peter},
doi = {10.1136/bmj.39609.449676.25},
file = {:home/rc538/Documents/References/Bmj/Hippisley-Cox et al. - 2008 - Predicting cardiovascular risk in England and Wales prospective derivation and validation of QRISK2.pdf:pdf},
isbn = {0959-8146},
issn = {0959-8138},
journal = {Bmj},
number = {7659},
pages = {1475--1482},
pmid = {18573856},
title = {{Predicting cardiovascular risk in England and Wales: prospective derivation and validation of QRISK2.}},
url = {https://qrisk.org/2016/BMJ-QRISK2.pdf},
volume = {336},
year = {2008}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/rc538/Documents/References/Unknown/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{McAuley2013,
abstract = {In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wiz- ards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we ob- tain highly interpretable textual labels for latent rating dimensions, which helps us to ‘justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the informa- tion present in review text; this is especially true for new products and users, who may have too few ratings to model their latent fac- tors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.},
author = {McAuley, J and Leskovec, Jure},
doi = {10.1145/2507157.2507163},
file = {:home/rc538/Documents/References/Proceedings of the 7th ACM conference on Recommender systems - RecSys '13/McAuley, Leskovec - 2013 - Hidden factors and hidden topics understanding rating dimensions with review text.pdf:pdf},
isbn = {9781450324090},
journal = {Proceedings of the 7th ACM conference on Recommender systems - RecSys '13},
keywords = {recommender systems,topic models},
pages = {165--172},
title = {{Hidden factors and hidden topics: understanding rating dimensions with review text}},
url = {http://i.stanford.edu/{~}julian/pdfs/recsys13.pdf http://dl.acm.org/citation.cfm?id=2507163},
year = {2013}
}
@misc{Hodge2004,
abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
archivePrefix = {arXiv},
arxivId = {ISSN 1573-7462},
author = {Hodge, Victoria J. and Austin, Jim},
booktitle = {Artificial Intelligence Review},
doi = {10.1023/B:AIRE.0000045502.10941.a9},
eprint = {ISSN 1573-7462},
file = {:home/rc538/Documents/References/Artificial Intelligence Review/Hodge, Austin - 2004 - A survey of outlier detection methodologies.pdf:pdf},
isbn = {1856044637},
issn = {02692821},
keywords = {Anomaly,Detection,Deviation,Noise,Novelty,Outlier,Recognition},
month = {oct},
number = {2},
pages = {85--126},
pmid = {3800},
publisher = {Kluwer Academic Publishers},
title = {{A survey of outlier detection methodologies}},
url = {http://link.springer.com/10.1023/B:AIRE.0000045502.10941.a9},
volume = {22},
year = {2004}
}
@article{Huopaniemi2014,
abstract = {Electronic medical records (EMR) contain a longitudinal collection of laboratory data that contains valuable phenotypic information on disease progression of a large collection of patients. These data can be potentially used in medical research or patient care; finding disease progression subtypes is a particularly important application. There are, however, two significant difficulties in utilizing this data for statistical analysis: (a) a large proportion of data is missing and (b) patients are in very different stages of disease progression and there are no well-defined start points of the time series. We present a Bayesian machine learning model that overcomes these difficulties. The method can use highly incomplete time-series measurement of varying lengths, it aligns together similar trajectories in different phases and is capable of finding consistent disease progression subtypes. We demonstrate the method on finding chronic kidney disease progression subtypes.},
author = {Huopaniemi, Ilkka and Nadkarni, Girish and Nadukuru, Rajiv and Lotay, Vaneet and Ellis, Steve and Gottesman, Omri and Bottinger, Erwin P},
file = {:home/rc538/Documents/References/AMIA ... Annual Symposium proceedings. AMIA Symposium/Huopaniemi et al. - 2014 - Disease progression subtype discovery from longitudinal EMR data with a majority of missing values and unknow.pdf:pdf},
issn = {1942-597X},
journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
pages = {709--18},
pmid = {25954377},
publisher = {American Medical Informatics Association},
title = {{Disease progression subtype discovery from longitudinal EMR data with a majority of missing values and unknown initial time points.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25954377 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4419979},
volume = {2014},
year = {2014}
}
@article{Spilka2017,
abstract = {Fetal heart rate (FHR) monitoring is routinely used in clinical practice to help obstetricians assess fetal health status during delivery. However, early detection of fetal acidosis that allows relevant decisions for operative delivery remains a challenging task, receiving considerable attention. This contribution promotes sparse support vector machine classification that permits to select a small number of relevant features and to achieve efficient fetal acidosis detection. A comprehensive set of features is used for FHR description, including enhanced and computerized clinical features, frequency domain, and scaling and multifractal features, all computed on a large (1288 subjects) and well-documented database. The individual performance obtained for each feature independently is discussed first. Then, it is shown that the automatic selection of a sparse subset of features achieves satisfactory classification performance (sensitivity 0.73 and specificity 0.75, outperforming clinical practice). The subset of selected features (average depth of decelerations MADdtrd, baseline level $\beta$0 , and variability H) receives simple interpretation in clinical practice. Intrapartum fetal acidosis detection is improved in several respects: A comprehensive set of features combining clinical, spectral, and scale-free dynamics is used; an original multivariate classification targeting both sparse feature selection and high performance is devised; state-of-the-art performance is obtained on a much larger database than that generally studied with description of common pitfalls in supervised classification performance assessments.},
author = {Spilka, Jiri and Frecon, Jordan and Leonarduzzi, Roberto and Pustelnik, Nelly and Abry, Patrice and Doret, Muriel},
doi = {10.1109/JBHI.2016.2546312},
file = {:home/rc538/Documents/References/IEEE Journal of Biomedical and Health Informatics/Spilka et al. - 2017 - Sparse Support Vector Machine for Intrapartum Fetal Heart Rate Classification.pdf:pdf},
isbn = {2168-2208 (Electronic)$\backslash$r2168-2194 (Linking)},
issn = {2168-2208},
journal = {IEEE Journal of Biomedical and Health Informatics},
month = {may},
number = {3},
pages = {664--671},
pmid = {27046884},
title = {{Sparse Support Vector Machine for Intrapartum Fetal Heart Rate Classification.}},
url = {http://ieeexplore.ieee.org/document/7440787/ http://www.ncbi.nlm.nih.gov/pubmed/27046884},
volume = {21},
year = {2017}
}
@misc{Fazel2016,
abstract = {Reply by the current authors to the comments made by Derek W Braverman et al. (see record 2016-43093-015) on the original article (see record 2016-18901-001). In our paper, we used transparent and robust methods to derive and externally validate a short, scalable tool (OxRec) for the prediction of violent reoffending in released prisoners. We made it clear that it should be used as an adjunct to professional judgement because individual factors and circumstances might need to be considered. Braverman and colleagues' comment that inclusion of socioeconomic variables is discriminatory. As others have pointed out, this would entirely depend on the baseline criminal justice context—in other words, all things being equal, what effect does using a potential risk calculator have on the system as it is currently practiced? (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Fazel, Seena and Chang, Zheng and L{\aa}ngstr{\"{o}}m, Niklas and Fanshawe, Thomas and Mallett, Susan},
booktitle = {The Lancet Psychiatry},
doi = {10.1016/S2215-0366(16)30216-4},
file = {:home/rc538/Documents/References/The Lancet Psychiatry/Fazel et al. - 2016 - OxRec model for assessing risk of recidivism Ethics Authors' reply.pdf:pdf},
isbn = {2215-0374(Electronic);2215-0366(Print)},
keywords = {*Institutional Release,*Prisoners,*Recidivism,*Violent Crime,Risk Assessment},
number = {9},
pages = {809--810},
title = {{"OxRec model for assessing risk of recidivism: Ethics": Authors' reply.}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5047350/pdf/emss-69812.pdf},
volume = {3},
year = {2016}
}
@article{Guzella2009,
abstract = {In this paper, we present a comprehensive review of recent developments in the application of machine learning algorithms to Spam filtering, focusing on both textual- and image-based approaches. Instead of considering Spam filtering as a standard classification problem, we highlight the importance of considering specific characteristics of the problem, especially concept drift, in designing new filters. Two particularly important aspects not widely recognized in the literature are discussed: the difficulties in updating a classifier based on the bag-of-words representation and a major difference between two early naive Bayes models. Overall, we conclude that while important advancements have been made in the last years, several aspects remain to be explored, especially under more realistic evaluation settings.},
author = {Guzella, Thiago S. and Caminhas, Walmir M.},
doi = {10.1016/j.eswa.2009.02.037},
file = {:home/rc538/Documents/References/Expert Systems with Applications/Guzella, Caminhas - 2009 - A review of machine learning approaches to Spam filtering.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
number = {7},
pages = {10206--10222},
title = {{A review of machine learning approaches to Spam filtering}},
url = {http://www.sciencedirect.com/science/article/pii/S095741740900181X},
volume = {36},
year = {2009}
}
@article{Halpern2016,
author = {Halpern, Yoni and Horng, Steven and Choi, Youngduck and Sontag, David},
doi = {10.1093/jamia/ocw011},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association/Halpern et al. - 2016 - Electronic medical record phenotyping using the anchor and learn framework.pdf:pdf},
issn = {1527974X},
journal = {Journal of the American Medical Informatics Association},
keywords = {clinical decision support systems,electronic health records,knowledge representation,machine learning,natural language processing},
number = {4},
pmid = {27107443},
title = {{Electronic medical record phenotyping using the anchor and learn framework}},
url = {https://oup.silverchair-cdn.com/oup/backfile/Content{\_}public/Journal/jamia/23/4/10.1093{\_}jamia{\_}ocw011/2/ocw011.pdf?Expires=1494335872{\&}Signature=NMX6WgrEAA2INVpNFWDfNtXMEu{~}CKxoZ4BMv-Zt{~}y5rZjdCo0BhQ4P9nOTb3dSvE2UVCh6lPb{~}k-aeSy5dQZ7udochflTypwk7L76OgbzYJpWPXWE},
volume = {23},
year = {2016}
}
@article{Lasko2013,
abstract = {Inferring precise phenotypic patterns from population-scale clinical data is a core computational task in the development of precision, personalized medicine. The traditional approach uses supervised learning, in which an expert designates which patterns to look for (by specifying the learning task and the class labels), and where to look for them (by specifying the input variables). While appropriate for individual tasks, this approach scales poorly and misses the patterns that we don't think to look for. Unsupervised feature learning overcomes these limitations by identifying patterns (or features) that collectively form a compact and expressive representation of the source data, with no need for expert input or labeled examples. Its rising popularity is driven by new deep learning methods, which have produced high-profile successes on difficult standardized problems of object recognition in images. Here we introduce its use for phenotype discovery in clinical data. This use is challenging because the largest source of clinical data – Electronic Medical Records – typically contains noisy, sparse, and irregularly timed observations, rendering them poor substrates for deep learning methods. Our approach couples dirty clinical data to deep learning architecture via longitudinal probability densities inferred using Gaussian process regression. From episodic, longitudinal sequences of serum uric acid measurements in 4368 individuals we produced continuous phenotypic features that suggest multiple population subtypes, and that accurately distinguished (0.97 AUC) the uric-acid signatures of gout vs. acute leukemia despite not being optimized for the task. The unsupervised features were as accurate as gold-standard features engineered by an expert with complete knowledge of the domain, the classification task, and the class labels. Our findings demonstrate the potential for achieving computational phenotype discovery at population scale. We expect such data-driven phenotypes to expose unknown disease variants and subtypes and to provide rich targets for genetic association studies. Citation: Lasko TA, Denny JC, Levy MA (2013) Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data. PLoS ONE 8(6): e66341.},
author = {Lasko, Thomas A. and Denny, Joshua C. and Levy, Mia A.},
doi = {10.1371/journal.pone.0066341},
file = {:home/rc538/Documents/References/PLoS ONE/Lasko, Denny, Levy - 2013 - Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clin.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {6},
pmid = {23826094},
title = {{Computational Phenotype Discovery Using Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical Data}},
url = {http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0066341{\&}type=printable},
volume = {8},
year = {2013}
}
@book{Society2017,
abstract = {Machine learning is a branch of artificial intelligence that allows computer systems to learn directly from examples, data, and experience. Through enabling computers to perform specific tasks intelligently, machine learning systems can carry out complex processes by learning from data, rather than following pre-programmed rules. Recent years have seen exciting advances in machine learning, which have raised its capabilities across a suite of applications. Increasing data availability has allowed machine learning systems to be trained on a large pool of examples, while increasing computer processing power has supported the analytical capabilities of these systems. Within the field itself there have also been algorithmic advances, which have given machine learning greater power. As a result of these advances, systems which only a few years ago performed at noticeably below-human levels can now outperform humans at some specific tasks. Many people now interact with systems based on machine learning every day, for example in image recognition systems, such as those used on social media; voice recognition systems, used by virtual personal assistants; and recommender systems, such as those used by online retailers. As the field develops further, machine learning shows promise of supporting potentially transformative advances in a range of areas, and the social and economic opportunities which follow are significant. In healthcare, machine learning is creating systems that can help doctors give more accurate or effective diagnoses for certain conditions. In transport, it is supporting the development of autonomous vehicles, and helping to make existing transport networks more efficient. For public services it has the potential to target support more effectively to those in need, or to tailor services to users. And in science, machine learning is helping to make sense of the vast amount of data available to researchers today, offering new insights into biology, physics, medicine, the social sciences, and more. The UK has a strong history of leadership in machine learning. From early thinkers in the field, through to recent commercial successes, the UK has supported excellence in research, which has contributed to the recent advances in machine learning that promise such potential. These strengths in research and development mean that the UK is well placed to take a leading role in the future development of machine learning. Ensuring the best possible environment for the safe and rapid deployment of machine learning will be essential for enhancing the UK's economic growth, wellbeing, and security, and for unlocking the value of ‘big data'. Action in key areas – shaping the data landscape, building skills, supporting business, and advancing research – can help create this environment},
author = {Society, The Royal},
file = {:home/rc538/Documents/References/Unknown/Society - 2017 - Machine learning the power and promise of computers that learn by example.pdf:pdf},
isbn = {9781782522591},
pages = {125},
title = {{Machine learning : the power and promise of computers that learn by example}},
url = {https://royalsociety.org/{~}/media/policy/projects/machine-learning/publications/machine-learning-report.pdf},
year = {2017}
}
@article{Goodman2016,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
eprint = {1606.08813},
file = {:home/rc538/Documents/References/2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)/Goodman, Flaxman - 2016 - EU regulations on algorithmic decision-making and a right to explanation.pdf:pdf},
journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
keywords = {machine learning},
month = {jun},
number = {Whi},
pages = {26--30},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
url = {http://arxiv.org/abs/1606.08813},
year = {2016}
}
@article{Miller1994,
abstract = {Articles about medical diagnostic decision support (MDDS) systems often begin with a disclaimer such as, "despite many years of research and millions of dollars of expenditures on medical diagnostic systems, none is in widespread use at the present time." While this statement remains true in the sense that no single diagnostic system is in widespread use, it is misleading with regard to the state of the art of these systems. Diagnostic systems, many simple and some complex, are now ubiquitous, and research on MDDS systems is growing. The nature of MDDS systems has diversified over time. The prospects for adoption of large-scale diagnostic systems are better now than ever before, due to enthusiasm for implementation of the electronic medical record in academic, commercial, and primary care settings. Diagnostic decision support systems have become an established component of medical technology. This paper provides a review and a threaded bibliography for some of the important work on MDDS systems over the years from 1954 to 1993.},
author = {Miller, R A},
doi = {10.1136/jamia.1994.95236141},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association JAMIA/Miller - 1994 - Medical diagnostic decision support systems--past, present, and future a threaded bibliography and brief commentary.pdf:pdf},
isbn = {10.1136/jamia.1994.95236141},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association : JAMIA},
number = {1},
pages = {8--27},
pmid = {7719792},
publisher = {American Medical Informatics Association},
title = {{Medical diagnostic decision support systems--past, present, and future: a threaded bibliography and brief commentary.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7719792 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC116181 http://www.ncbi.nlm.nih.gov/pubmed/7719792{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC116181},
volume = {1},
year = {1994}
}
@article{Nadkarni2011,
abstract = {OBJECTIVES To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. TARGET AUDIENCE This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art. SCOPE We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field.},
author = {Nadkarni, Prakash M and Ohno-Machado, Lucila and Chapman, Wendy W and Manning, C. and Raghavan, P. and Schuetze, H. and Hutchins, W. and Chomsky, N. and Aho, AV. and Sethi, R. and Ullman, JD. and Chomsky, N. and Friedl, JEF. and Kleene, SC. and Kernighan, B. and Pike, R. and Levine, JR. and Mason, T. and Brown, D. and Joshi, A. and Vijay-Shanker, K. and Weir, D. and Clocksin, WF. and Mellish, CS. and Warren, DS. and Klein, D. and Chomsky, N. and Klein, D. and Manning, C. and Quinlan, JR. and Klein, D. and Manning, C. and of Pennsylvania, University and Manning, C. and Schuetze, H. and Jurafsky, D. and Martin, JH. and Spyns, P. and Deleger, L. and Namer, F. and Zweigenbaum, P. and Denny, JC. and Spickard, A. and Johnson, KB. and Haas, S. and Savova, GK. and Masanz, JJ. and Ogren, PV. and Aronson, A. and Zou, Q. and Chu, WW. and Morioka, C. and Liu, H. and Aronson, A. and Friedman, C. and Rindflesch, TC. and Aronson, AR. and Pedersen, T. and Weeber, M. and Mork, JG. and Aronson, AR. and Chapman, W. and Bridewell, W. and Hanbury, P. and Mutalik, P. and Deshpande, A. and Nadkarni, P. and Huang, Y. and Lowe, HJ. and Chapman, W. and Bridewell, W. and Hanbury, P. and (Hungary), University of Szeged and Savova, GK. and Chapman, WW. and Zheng, J. and Tao, C. and Solbrig, H. and Deepak, S. and Hripcsak, G. and Elhadad, N. and Chen, YH. and Taira, RK. and Johnson, DB. and Bhushan, V. and Sager, N. and Lyman, M. and Nhan, N. and Haug, PJ. and Ranum, DL. and Frederick, PR. and Christensen, L. and Haug, PJ. and Fiszman, M. and Xu, H. and Friedman, C. and Stetson, PD. and Christensen, L. and Harkema, H. and Irwin, J. and Uzuner, O. and South, B. and Shen, S. and Uzuner, O. and Goldstein, I. and Luo, Y. and Uzuner, O. and Uzuner, O. and Solti, I. and Cadag, E. and Chute, C. and Chen, L. and Friedman, C. and Wang, X. and Hripcsak, G. and Friedman, C. and Chapman, WW. and Fiszman, M. and Dowling, JN. and Chapman, WW. and Dowling, JN. and Wagner, MM. and Wang, X. and Hripcsak, G. and Markatou, M. and Lindberg, DAB. and Humphreys, BL. and McCray, AT. and Browne, AC. and McCray, AT. and Srinivasan, S. and Divita, G. and Browne, AC. and Rindflesch, TC. and of Medicine, National Library and Srihari, S. and Elkan, C. and Hearst, MA. and Dumais, ST. and Osman, E. and Inc., DTREG and Fradkin, D. and Muchnik, I. and Viterbi, A. and Dempster, AP. and Laird, NM. and Rubin, DB. and Hasegawa-Johnson, M. and Zhang, J. and Shen, D. and Zhou, G. and Sonnhammer, ELL. and Eddy, SR. and Birney, E. and Lukashin, A. and Borodovsky, M. and Eddy, SR. and Rabiner, LR. and Lafferty, J. and McCallum, A. and Pereira, F. and Culotta, A. and Kulp, D. and McCallum, A. and Sutton, C. and McCallum, A. and Franz, A. and Brants, T. and Group, University of Sheffield Natural Language and Foundation, Apache and Foundation, Apache and Thompson, C. and Califf, M. and Mooney, R. and Linguistics, North American Chapter of the Association for Computational and Fodor, P. and Lally, A. and Ferrucci, D. and Lally, A. and Fodor, P. and Libresco, LA. and Jennings, K. and Fitzgerald, J. and Berner, ES. and Webster, GD. and Shugerman, AA. and Friedman, CP. and Elstein, AS. and Wolf, FM. and Miller, R. and Masarie, F. and Miller, R.},
doi = {10.1136/amiajnl-2011-000464},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association JAMIA/Nadkarni et al. - 2011 - Natural language processing an introduction.pdf:pdf},
isbn = {9783662459232},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association : JAMIA},
month = {sep},
number = {5},
pages = {544--51},
pmid = {21846786},
publisher = {Oxford University Press},
title = {{Natural language processing: an introduction.}},
url = {https://academic.oup.com/jamia/article-lookup/doi/10.1136/amiajnl-2011-000464 http://www.ncbi.nlm.nih.gov/pubmed/21846786{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3168328},
volume = {18},
year = {2011}
}
@article{Rosenbloom2010,
abstract = {Clinical notes summarize interactions that occur between patients and healthcare providers. With adoption of electronic health record (EHR) and computer-based documentation (CBD) systems, there is a growing emphasis on structuring clinical notes to support reusing data for subsequent tasks. However, clinical documentation remains one of the most challenging areas for EHR system development and adoption. The current manuscript describes the Vanderbilt experience with implementing clinical documentation with an EHR system. Based on their experience rolling out an EHR system that supports multiple methods for clinical documentation, the authors recommend that documentation method selection be made on the basis of clinical workflow, note content standards and usability considerations, rather than on a theoretical need for structured data.},
author = {Rosenbloom, S Trent and Stead, William W and Denny, Joshua C and Giuse, Dario and Lorenzi, Nancy M and Brown, Steven H and Johnson, Kevin B},
doi = {10.4338/ACI-2010-03-RA-0019},
file = {:home/rc538/Documents/References/Appl Clin Inform/Rosenbloom et al. - 2010 - Generating Clinical Notes for Electronic Health Record Systems.pdf:pdf},
isbn = {1869-0327 (Print)},
issn = {1869-0327 (Electronic)},
journal = {Appl Clin Inform},
keywords = {compute-,computer based documentation,electronic h},
month = {jan},
number = {3},
pages = {232--243},
pmid = {21031148},
publisher = {NIH Public Access},
title = {{Generating Clinical Notes for Electronic Health Record Systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21031148 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2963994},
volume = {1},
year = {2010}
}
@article{McSharry2003,
abstract = {A dynamical model based on three coupled ordinary differential equations is introduced which is capable of generating realistic synthetic electrocardiogram (ECG) signals. The operator can specify the mean and standard deviation of the heart rate, the morphology of the PQRST cycle, and the power spectrum of the RR tachogram. In particular, both respiratory sinus arrhythmia at the high frequencies (HFs) and Mayer waves at the low frequencies (LFs) together with the LF/HF ratio are incorporated in the model. Much of the beat-to-beat variation in morphology and timing of the human ECG, including QT dispersion and R-peak amplitude modulation are shown to result. This model may be employed to assess biomedical signal processing techniques which are used to compute clinical statistics from the ECG.},
author = {McSharry, Patrick E. and Clifford, Gari D. and Tarassenko, Lionel and Smith, Leonard A.},
doi = {10.1109/TBME.2003.808805},
file = {:home/rc538/Documents/References/IEEE Transactions on Biomedical Engineering/McSharry et al. - 2003 - A dynamical model for generating synthetic electrocardiogram signals.pdf:pdf},
isbn = {0018-9294},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Dynamical model,Heart rate variability (HRV),Mayer waves,QRS morphology,QT-interval,RR tachogram,RR-interval,Respiratory sinus arrhythmia,Synthetic ECG},
month = {mar},
number = {3},
pages = {289--294},
pmid = {12669985},
title = {{A dynamical model for generating synthetic electrocardiogram signals}},
url = {http://ieeexplore.ieee.org/document/1186732/},
volume = {50},
year = {2003}
}
@article{Chen2016,
abstract = {Objective: Build probabilistic topic model representations of hospital admissions processes and compare the ability of such models to predict clinical order patterns as compared to preconstructed order sets. Materials and Methods: The authors evaluated the first 24 hours of structured electronic health record data for {\textgreater}10K inpatients. Drawing an analogy between structured items (e.g., clinical orders) to words in a text docu- ment, the authors performed latent Dirichlet allocation probabilistic topic modeling. These topic models use initial clinical information to predict clinical orders for a separate validation set of {\textgreater}4K patients. The authors evaluated these topic model-based predictions vs existing human-authored order sets by area under the receiver operating characteristic curve, precision, and recall for subsequent clinical orders. Results: Existing order sets predict clinical orders used within 24 hours with area under the receiver operating characteristic curve 0.81, precision 16{\%}, and recall 35{\%}. This can be improved to 0.90, 24{\%}, and 47{\%} (P{\textless}10?20) by using probabilistic topic models to summarize clinical data into up to 32 topics. Many of these latent topics yield natural clinical interpretations (e.g., “critical care,” “pneumonia,” “neurologic evaluation”). Discussion: Existing order sets tend to provide nonspecific, process-oriented aid, with usability limitations impairing more precise, patient-focused support. Algorithmic summarization has the potential to breach this usability barrier by automatically inferring patient context, but with potential tradeoffs in interpretability. Conclusion: Probabilistic topic modeling provides an automated approach to detect thematic trends in patient care and generate decision support content. A potential use case finds related clinical orders for decision support.},
author = {Chen, Jonathan H and Goldstein, Mary K and Asch, Steven M and Mackey, Lester and Altman, Russ B},
doi = {10.1093/jamia/ocw136},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association/Chen et al. - 2016 - Predicting inpatient clinical order patterns with probabilistic topic models vs conventional order sets.pdf:pdf},
isbn = {1067-5027},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
keywords = {clinical decision support systems,clinical summarization,data mining,electronic health records,order sets,probabilistic topic modeling},
month = {sep},
number = {3},
pages = {ocw136},
pmid = {27655861},
publisher = {Oxford University Press},
title = {{Predicting inpatient clinical order patterns with probabilistic topic models vs conventional order sets}},
url = {https://academic.oup.com/jamia/article-lookup/doi/10.1093/jamia/ocw136 http://jamia.oxfordjournals.org/lookup/doi/10.1093/jamia/ocw136},
volume = {22},
year = {2016}
}
@book{EthemAlpaydin,
author = {{Ethem Alpaydin}},
title = {{Machine Learning: The New AI (The MIT Press Essential Knowledge Series)}}
}
@article{Kasteridis2015,
abstract = {Conclusions: The linked dataset makes it possible to understand existing patterns of health and social care utilisation and to analyse variation in annual costs, for the whole population and for sub-groups, in total and by setting. This has made it possible to identify who would most benefit from improved integrated care and to calculate capitated budgets to support financial integration. Aims: The Symphony Project is designed to identify which groups of the South Somerset population in England would most benefit from greater integration across primary, community, acute and social care settings. Methods: We analysed linked health and social care data for the entire South Somerset population for the financial year 2012/2013. The data captured acute, primary, community, mental health and social care utilisation and costs; demographic characteristics; and indicators of morbidity for each individual. We employed generalized linear models to analyse variation in annual health and social care costs for all 114,874 members of the South Somerset population and for 1458 individuals with three or more selected chronic conditions. Results: We found that multi-morbidity, not age, was the key driver of health and social care costs. Moreover, the number of chronic conditions is as useful as information about specific conditions at predicting costs. We are able to explain 7{\%} of the variation in total annual costs for population as a whole, and 14{\%} of the variation for those with three or more conditions. We are best able to explain primary care costs, but explanatory power is poor for mental health and social care costs.},
author = {Kasteridis, Panagiotis and Street, Andrew and Dolman, Matthew and Gallier, Lesley and Hudson, Kevin and Martin, Jeremy and Wyer, Ian},
doi = {10.1136/bmj.b1484},
file = {:home/rc538/Documents/References/International Journal of Integrated Care/Kasteridis et al. - 2015 - Who would most benefit from improved integrated care Implementing an analytical strategy in South Somerset.pdf:pdf},
isbn = {1568-4156 (Electronic)},
issn = {1568-4156},
journal = {International Journal of Integrated Care},
keywords = {Capitated budgets,Health and social care costs,Integrated care,Morbidity profiles},
number = {January},
pages = {1--11},
pmid = {19429651},
publisher = {Ubiquity Press},
title = {{Who would most benefit from improved integrated care? Implementing an analytical strategy in South Somerset}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25674043 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4314133 http://www.scopus.com/inward/record.url?eid=2-s2.0-84921966568{\&}partnerID=40{\&}md5=87482eb352228c5d7503b1fc9e355b79},
volume = {15},
year = {2015}
}
@article{Price2016,
abstract = {OBJECTIVES: To estimate data loss and bias in studies of Clinical Practice Research Datalink (CPRD) data that restrict analyses to Read codes, omitting anything recorded as text. DESIGN: Matched case-control study. SETTING: Patients contributing data to the CPRD. PARTICIPANTS: 4915 bladder and 3635 pancreatic, cancer cases diagnosed between 1 January 2000 and 31 December 2009, matched on age, sex and general practitioner practice to up to 5 controls (bladder: n=21 718; pancreas: n=16 459). The analysis period was the year before cancer diagnosis. PRIMARY AND SECONDARY OUTCOME MEASURES: Frequency of haematuria, jaundice and abdominal pain, grouped by recording style: Read code or text-only (ie, hidden text). The association between recording style and case-control status (chi(2) test). For each feature, the odds ratio (OR; conditional logistic regression) and positive predictive value (PPV; Bayes' theorem) for cancer, before and after addition of hidden text records. RESULTS: Of the 20 958 total records of the features, 7951 (38{\%}) were recorded in hidden text. Hidden text recording was more strongly associated with controls than with cases for haematuria (140/336=42{\%} vs 556/3147=18{\%}) in bladder cancer (chi(2) test, p{\textless}0.001), and for jaundice (21/31=67{\%} vs 463/1565=30{\%}, p{\textless}0.0001) and abdominal pain (323/1126=29{\%} vs 397/1789=22{\%}, p{\textless}0.001) in pancreatic cancer. Adding hidden text records corrected PPVs of haematuria for bladder cancer from 4.0{\%} (95{\%} CI 3.5{\%} to 4.6{\%}) to 2.9{\%} (2.6{\%} to 3.2{\%}), and of jaundice for pancreatic cancer from 12.8{\%} (7.3{\%} to 21.6{\%}) to 6.3{\%} (4.5{\%} to 8.7{\%}). Adding hidden text records did not alter the PPV of abdominal pain for bladder (codes: 0.14{\%}, 0.13{\%} to 0.16{\%} vs codes plus hidden text: 0.14{\%}, 0.13{\%} to 0.15{\%}) or pancreatic (0.23{\%}, 0.21{\%} to 0.25{\%} vs 0.21{\%}, 0.20{\%} to 0.22{\%}) cancer. CONCLUSIONS: Omission of text records from CPRD studies introduces bias that inflates outcome measures for recognised alarm symptoms. This potentially reinforces clinicians' views of the known importance of these symptoms, marginalising the significance of 'low-risk but not no-risk' symptoms.},
author = {Price, Sarah J and Stapley, Sal A and Shephard, Elizabeth and Barraclough, Kevin and Hamilton, William T},
doi = {10.1136/bmjopen-2016-011664},
file = {:home/rc538/Documents/References/BMJ Open/Price et al. - 2016 - Is omission of free text records a possible source of data loss and bias in Clinical Practice Research Datalink st.pdf:pdf},
isbn = {2044-6055 (Electronic)$\backslash$r2044-6055 (Linking)},
issn = {2044-6055},
journal = {BMJ Open},
keywords = {ONCOLOGY,PRIMARY CARE,STATISTICS {\&} RESEARCH METHODS},
month = {may},
number = {5},
pages = {e011664},
pmid = {27178981},
publisher = {BMJ Group},
title = {{Is omission of free text records a possible source of data loss and bias in Clinical Practice Research Datalink studies? A case-control study}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27178981 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4874123},
volume = {6},
year = {2016}
}
@article{Shephard2016,
abstract = {BACKGROUND: Leukaemia is the eleventh commonest UK cancer. The four main subtypes have different clinical profiles, particularly between chronic and acute types. AIM: To identify the symptom profiles of chronic and acute leukaemia in adults in primary care. DESIGN AND SETTING: Matched case-control studies using Clinical Practice Research Datalink records. METHOD: Putative symptoms of leukaemia were identified in the year before diagnosis. Conditional logistic regression was used for analysis, and positive predictive values (PPVs) were calculated to estimate risk. RESULTS: Of cases diagnosed between 2000 and 2009, 4655 were aged {\textgreater}/=40 years (2877 chronic leukaemia (CL), 937 acute leukaemia (AL), 841 unreported subtype). Ten symptoms were independently associated with CL, the three strongest being: lymphadenopathy (odds ratio [OR] 22, 95{\%} confidence interval [CI] = 13 to 36), weight loss (OR 3.0, 95{\%} CI = 2.1 to 4.2), and bruising (OR 2.3, 95{\%} CI = 1.6 to 3.2). Thirteen symptoms were independently associated with AL, the three strongest being: nosebleeds and/or bleeding gums (OR 5.7, 95{\%} CI = 3.1 to 10), fever (OR 5.3, 95{\%} CI = 2.7 to 10), and fatigue (OR 4.4, 95{\%} CI = 3.3 to 6.0). No individual symptom or combination of symptoms had a PPV {\textgreater}1{\%}. CONCLUSION: The symptom profiles of CL and AL have both overlapping and distinct features. This presents a dichotomy for GPs: diagnosis, by performing a full blood count, is easy; however, the symptoms of leukaemia are non-specific and of relatively low risk. This explains why many leukaemia diagnoses are unexpected findings.},
author = {Shephard, Elizabeth A and Neal, Richard D and Rose, Peter W and Walter, Fiona M and Hamilton, Willie},
doi = {10.3399/bjgp16X683989},
file = {:home/rc538/Documents/References/British Journal of General Practice/Shephard et al. - 2016 - Symptoms of adult chronic and acute leukaemia before diagnosis large primary care case-control studies using el.pdf:pdf},
isbn = {0960-1643},
issn = {0960-1643},
journal = {British Journal of General Practice},
keywords = {acute lymphoblastic leukaemia,acute myeloid leukaemia,chronic leukaemia,diagnosis,primary health care},
month = {mar},
number = {644},
pages = {e182--8},
pmid = {26917658},
publisher = {Royal College of General Practitioners},
title = {{Symptoms of adult chronic and acute leukaemia before diagnosis: large primary care case-control studies using electronic records}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26917658 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4758497},
volume = {66},
year = {2016}
}
@article{Ravi2017,
abstract = {The increasing popularity of wearable devices in recent years means that a diverse range of physiologi- cal and functional data can now be captured continuously for applications in sports, wellbeing, and healthcare. This wealth of information requires efficient methods of clas- sification and analysis where deep learning is a promis- ing technique for large-scale data analytics. While deep learning has been successful in implementations that uti- lize high-performance computing platforms, its use on low- power wearable devices is limited by resource constraints. In this paper, we propose a deep learning methodology, which combines features learned from inertial sensor data togetherwith complementary information froma set of shal- low features to enable accurate and real-time activity clas- sification. The design of this combined method aims to overcome some of the limitations present in a typical deep learning frameworkwhere on-node computation is required. To optimize the proposed method for real-time on-node computation, spectral domain preprocessing is used before the data are passed onto the deep learning framework. The classification accuracy of our proposed deep learning ap- proach is evaluated against state-of-the-art methods using both laboratory and real world activity datasets. Our results show the validity of the approach on different human ac- tivity datasets, outperforming other methods, including the two methods used within our combined pipeline. We also demonstrate that the computation times for the proposed method are consistent with the constraints of real-time on- node processing on smartphones and a wearable sensor platform. Index},
author = {Ravi, Daniele and Wong, Charence and Lo, Benny and Yang, Guang-Zhong},
doi = {10.1109/JBHI.2016.2633287},
file = {:home/rc538/Documents/References/IEEE Journal of Biomedical and Health Informatics/Ravi et al. - 2017 - A Deep Learning Approach to on-Node Sensor Data Analytics for Mobile or Wearable Devices.pdf:pdf},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {ActiveMiles,Human Activity Recognition (HAR),Internet-of-Things (IoT),deep learning,low-power devices,wearable.},
month = {jan},
number = {1},
pages = {56--64},
title = {{A Deep Learning Approach to on-Node Sensor Data Analytics for Mobile or Wearable Devices}},
url = {http://ieeexplore.ieee.org/document/7797232/},
volume = {21},
year = {2017}
}
@article{Chenore2013,
abstract = {BACKGROUND: In the UK, people aged 85 and over are the fastest growing population group and are predicted to double in number by 2030. Emergency hospital admissions are also rising. METHODS: All emergency admissions for the registered population in Devon to all English hospitals were analysed by age, and admission rates per thousand registered were calculated. The Devon Predictive Model (DPM) was built, using local data, to predict emergency admissions in the following 12 months. This model was compared with the Combined Predictive Model over five risk categories. RESULTS: The registered Devon population on 31 March 2011 was 761 652 with 65 892 emergency admissions in 2010/2011. The DPM had 89 variables including several local factors which strengthened the model. Three of the four most powerful predictors were age 85-89, 90-94 and 95 and over. The positive predictive value for the DPM was better than the CPM's in all five risk categories. Half (49.6{\%}) of all emergency admissions were from those aged 65 or over. Admissions rose progressively and significantly in each successive elderly age band. At age 85 and over there were 420 emergency admissions per thousand registered. CONCLUSIONS: Age, especially 85 and over, has been undervalued as a risk factor for emergency hospital admissions.},
author = {Chenore, T. and {Pereira Gray}, D. J. and Forrer, J. and Wright, C. and Evans, P. H.},
doi = {10.1093/pubmed/fdt009},
file = {:home/rc538/Documents/References/Journal of Public Health (United Kingdom)/Chenore et al. - 2013 - Emergency hospital admissions for the elderly Insights from the Devon Predictive Model.pdf:pdf},
issn = {17413842},
journal = {Journal of Public Health (United Kingdom)},
keywords = {Age,Age85plus,Emergency care,Hospitals,Predictive-modelling},
month = {dec},
number = {4},
pages = {616--623},
pmid = {23440706},
title = {{Emergency hospital admissions for the elderly: Insights from the Devon Predictive Model}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23440706 https://academic.oup.com/jpubhealth/article-lookup/doi/10.1093/pubmed/fdt009},
volume = {35},
year = {2013}
}
@article{Agarwal2017,
abstract = {—With the passage of recent federal legislation many medical institutions are now responsible for reaching target hospital readmission rates. Chronic diseases account for many hospital readmissions and Chronic Obstructive Pulmonary Disease has been recently added to the list of diseases for which the United States government penalizes hospitals incurring excessive readmissions. Though there have been efforts to statistically predict those most in danger of readmission, few have focused primarily on unstructured clinical notes. We have proposed a framework which uses Natural Language Processing to analyze clinical notes and predict readmission. Many algorithms within the field of data mining and machine learning exist, so a framework for component selection is created to select the best components. Na{\"{i}}ve Bayes using Chi-Squared feature selection offers an AUC of 0.690 while maintaining fast computational times. Keywords—Natural language processing, Medical information systems, Decision support systems, Data mining, Feature Extraction},
author = {Agarwal, Ankur and Baechle, Christopher and Behara, Ravi and Zhu, Xingquan},
doi = {10.1109/JBHI.2017.2684121},
file = {:home/rc538/Documents/References/IEEE Journal of Biomedical and Health Informatics/Agarwal et al. - 2017 - A Natural Language Processing Framework for Assessing Hospital Readmissions for Patients with COPD.pdf:pdf},
issn = {2168-2194},
journal = {IEEE Journal of Biomedical and Health Informatics},
pages = {1--1},
title = {{A Natural Language Processing Framework for Assessing Hospital Readmissions for Patients with COPD}},
url = {http://ieeexplore.ieee.org/document/7880658/ http://www.ieee.org/publications{\_}standards/publications/rights/index.html},
year = {2017}
}
@article{Chu2016,
abstract = {Radiotherapy is one of the main ways head and neck cancers are treated; radiation is used to kill cancerous cells and prevent their recurrence. Complex treatment planning is required to ensure that enough radiation is given to the tumour, and little to other sensitive structures (known as organs at risk) such as the eyes and nerves which might otherwise be damaged. This is especially difficult in the head and neck, where multiple at-risk structures often lie in extremely close proximity to the tumour. It can take radiotherapy experts four hours or more to pick out the important areas on planning scans (known as segmentation).},
author = {Chu, Carlton and {De Fauw}, Jeffrey and Tomasev, Nenad and Paredes, Bernardino Romera and Hughes, C{\'{i}}an and Ledsam, Joseph and Back, Trevor and Montgomery, Hugh and Rees, Geraint and Raine, Rosalind and Sullivan, Kevin and Moinuddin, Syed and D'Souza, Derek and Ronneberger, Olaf and Mendes, Ruheena and Cornebise, Julien},
doi = {10.12688/f1000research.9525.1},
file = {:home/rc538/Documents/References/F1000Research/Chu et al. - 2016 - Applying machine learning to automated segmentation of head and neck tumour volumes and organs at risk on radiothera.pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
pages = {2104},
title = {{Applying machine learning to automated segmentation of head and neck tumour volumes and organs at risk on radiotherapy planning CT and MRI scans}},
url = {https://f1000researchdata.s3.amazonaws.com/manuscripts/10262/8729e440-c564-4d58-84d4-74cdff3d5d0a{\_}9525{\_}-{\_}joseph{\_}ledsam.pdf?doi=10.12688/f1000research.9525.1 http://f1000research.com/articles/5-2104/v1},
volume = {5},
year = {2016}
}
@article{DeFauw2016,
abstract = {There are almost two million people in the United Kingdom living with sight loss, including around 360,000 people who are registered as blind or partially sighted. Sight threatening diseases, such as diabetic retinopathy and age related macular degeneration have contributed to the 40{\%} increase in outpatient attendances in the last decade but are amenable to early detection and monitoring. With early and appropriate intervention, blindness may be prevented in many cases. Ophthalmic imaging provides a way to diagnose and objectively assess the progression of a number of pathologies including neovascular (" wet ") age-related macular degeneration (wet AMD) and diabetic retinopathy. Two methods of imaging are commonly used: digital photographs of the fundus (the 'back' of the eye) and Optical Coherence Tomography (OCT, a modality that uses light waves in a similar way to how ultrasound uses sound waves). Changes in population demographics and expectations and the changing pattern of chronic diseases creates a rising demand for such imaging. Meanwhile, interrogation of such images is time consuming, costly, and prone to human error. The application of novel analysis methods may provide a solution to these challenges.},
author = {{De Fauw}, Jeffrey and Keane, Pearse and Tomasev, Nenad and Visentin, Daniel and van den Driessche, George and Johnson, Mike and Hughes, Cian O and Chu, Carlton and Ledsam, Joseph and Back, Trevor and Peto, Tunde and Rees, Geraint and Montgomery, Hugh and Raine, Rosalind and Ronneberger, Olaf and Cornebise, Julien},
doi = {10.12688/f1000research.8996.1},
file = {:home/rc538/Documents/References/F1000Research/De Fauw et al. - 2016 - Automated analysis of retinal imaging using machine learning techniques for computer vision.pdf:pdf},
isbn = {2046-1402},
issn = {2046-1402},
journal = {F1000Research},
pages = {1573},
title = {{Automated analysis of retinal imaging using machine learning techniques for computer vision}},
url = {https://f1000researchdata.s3.amazonaws.com/manuscripts/9679/31f2020f-d853-49f7-b3a8-ec93393c3510{\_}8996{\_}-{\_}joseph{\_}ledsam.pdf?doi=10.12688/f1000research.8996.1 http://f1000research.com/articles/5-1573/v1},
volume = {5},
year = {2016}
}
@article{Powles2017,
author = {Powles, Julia and Hodson, Hal},
doi = {10.1007/s12553-017-0179-1},
file = {:home/rc538/Documents/References/Health and Technology/Powles, Hodson - 2017 - Google DeepMind and healthcare in an age of algorithms.pdf:pdf},
isbn = {1255301701791},
issn = {2190-7188},
journal = {Health and Technology},
month = {mar},
pages = {1--17},
publisher = {Springer Berlin Heidelberg},
title = {{Google DeepMind and healthcare in an age of algorithms}},
url = {http://link.springer.com/10.1007/s12553-017-0179-1},
year = {2017}
}
@article{Li2015,
abstract = {BACKGROUND: In this study we implemented and developed state-of-the-art machine learning (ML) and natural language processing (NLP) technologies and built a computerized algorithm for medication reconciliation. Our specific aims are: (1) to develop a computerized algorithm for medication discrepancy detection between patients' discharge prescriptions (structured data) and medications documented in free-text clinical notes (unstructured data); and (2) to assess the performance of the algorithm on real-world medication reconciliation data.$\backslash$n$\backslash$nMETHODS: We collected clinical notes and discharge prescription lists for all 271 patients enrolled in the Complex Care Medical Home Program at Cincinnati Children's Hospital Medical Center between 1/1/2010 and 12/31/2013. A double-annotated, gold-standard set of medication reconciliation data was created for this collection. We then developed a hybrid algorithm consisting of three processes: (1) a ML algorithm to identify medication entities from clinical notes, (2) a rule-based method to link medication names with their attributes, and (3) a NLP-based, hybrid approach to match medications with structured prescriptions in order to detect medication discrepancies. The performance was validated on the gold-standard medication reconciliation data, where precision (P), recall (R), F-value (F) and workload were assessed.$\backslash$n$\backslash$nRESULTS: The hybrid algorithm achieved 95.0{\%}/91.6{\%}/93.3{\%} of P/R/F on medication entity detection and 98.7{\%}/99.4{\%}/99.1{\%} of P/R/F on attribute linkage. The medication matching achieved 92.4{\%}/90.7{\%}/91.5{\%} (P/R/F) on identifying matched medications in the gold-standard and 88.6{\%}/82.5{\%}/85.5{\%} (P/R/F) on discrepant medications. By combining all processes, the algorithm achieved 92.4{\%}/90.7{\%}/91.5{\%} (P/R/F) and 71.5{\%}/65.2{\%}/68.2{\%} (P/R/F) on identifying the matched and the discrepant medications, respectively. The error analysis on algorithm outputs identified challenges to be addressed in order to improve medication discrepancy detection.$\backslash$n$\backslash$nCONCLUSION: By leveraging ML and NLP technologies, an end-to-end, computerized algorithm achieves promising outcome in reconciling medications between clinical notes and discharge prescriptions.},
author = {Li, Qi and Spooner, Stephen Andrew and Kaiser, Megan and Lingren, Nataline and Robbins, Jessica and Lingren, Todd and Tang, Huaxiu and Solti, Imre and Ni, Yizhao},
doi = {10.1186/s12911-015-0160-8},
file = {:home/rc538/Documents/References/BMC medical informatics and decision making/Li et al. - 2015 - An end-to-end hybrid algorithm for automated medication discrepancy detection.pdf:pdf},
issn = {1472-6947},
journal = {BMC medical informatics and decision making},
keywords = {Automated medication reconciliation,Medication dis,automated medication reconciliation,machine learning,medication discrepancy detection,natural},
number = {1},
pages = {37},
pmid = {25943550},
title = {{An end-to-end hybrid algorithm for automated medication discrepancy detection.}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4427951/pdf/12911{\_}2015{\_}Article{\_}160.pdf http://www.biomedcentral.com/1472-6947/15/37},
volume = {15},
year = {2015}
}
@article{Ni2015,
abstract = {OBJECTIVES (1) To develop an automated eligibility screening (ES) approach for clinical trials in an urban tertiary care pediatric emergency department (ED); (2) to assess the effectiveness of natural language processing (NLP), information extraction (IE), and machine learning (ML) techniques on real-world clinical data and trials. DATA AND METHODS We collected eligibility criteria for 13 randomly selected, disease-specific clinical trials actively enrolling patients between January 1, 2010 and August 31, 2012. In parallel, we retrospectively selected data fields including demographics, laboratory data, and clinical notes from the electronic health record (EHR) to represent profiles of all 202795 patients visiting the ED during the same period. Leveraging NLP, IE, and ML technologies, the automated ES algorithms identified patients whose profiles matched the trial criteria to reduce the pool of candidates for staff screening. The performance was validated on both a physician-generated gold standard of trial-patient matches and a reference standard of historical trial-patient enrollment decisions, where workload, mean average precision (MAP), and recall were assessed. RESULTS Compared with the case without automation, the workload with automated ES was reduced by 92{\%} on the gold standard set, with a MAP of 62.9{\%}. The automated ES achieved a 450{\%} increase in trial screening efficiency. The findings on the gold standard set were confirmed by large-scale evaluation on the reference set of trial-patient matches. DISCUSSION AND CONCLUSION By exploiting the text of trial criteria and the content of EHRs, we demonstrated that NLP-, IE-, and ML-based automated ES could successfully identify patients for clinical trials.},
author = {Ni, Yizhao and Kennebeck, Stephanie and Dexheimer, Judith W and McAneney, Constance M and Tang, Huaxiu and Lingren, Todd and Li, Qi and Zhai, Haijun and Solti, Imre and Embi, PJ. and Jain, A. and Harris, CM. and Thadani, SR. and Weng, C. and Bigger, JT. and Embi, PJ. and Payne, PR. and Penberthy, LT. and Dahman, BA. and Petkov, VI. and Ibrahim, GM. and Chung, C. and Bernstein, M. and Wynn, L. and Miller, S. and Faughnan, L. and Dickson, M. and Gagnon, JP. and Butte, AJ. and Weinstein, DA. and Kohane, IS. and Embi, PJ. and Jain, A. and Clark, J. and Grundmeier, RW. and Swietlik, M. and Bell, LM. and Nkoy, FL. and Wolfe, D. and Hales, JW. and Treweek, S. and Pearson, E. and Smith, N. and Heinemann, S. and Th{\"{u}}ring, S. and Wedeken, S. and Pressler, TR. and Yen, PY. and Ding, J. and Ding, J. and Erdal, S. and Borlawsky, T. and Penberthy, L. and Brown, R. and Puma, F. and Embi, PJ. and Jain, A. and Clark, J. and Beauharnais, CC. and Larkin, ME. and Zai, AH. and Petkov, VI. and Penberthy, LT. and Dahman, BA. and Embi, PJ. and Leonard, AC. and Stanfill, MH. and Williams, M. and Fenton, SH. and Weng, C. and Tu, SW. and Sim, I. and Luo, Z. and Yetisgen-Yildiz, M. and Weng, C. and Weng, C. and Wu, X. and Luo, Z. and Tu, SW. and Peleg, M. and Carini, S. and Korkontzelos, I. and Mu, T. and Ananiadou, S. and Bhattacharya, S. and Cantor, MN. and Gurulingappa, H. and M{\"{u}}ller, B. and Hofmann-Apitius, M. and King, B. and Wang, L. and Provalov, I. and Limsopatham, N. and Macdonald, C. and Ounis, I. and Demner-Fushman, D. and Abhyankar, S. and Jimeno-Yepes, A. and Callejas, PMA. and Wang, Y. and Fang, H. and Limsophatham, N. and McCreadie, R. and Albakour, MD. and Qi, Y. and Laquerre, PF. and Zhu, D. and Chapman, WW. and Nadkarni, PM. and Hirschman, L. and Edinger, T. and Cohen, AM. and Bedrick, S. and Schmickl, CN. and Li, M. and Li, G. and Ogren, P. and Savova, G. and Chute, C. and Marneffe, M. De and MacCartney, B. and Manning, CD. and Savova, GK. and Masanz, JJ. and Ogren, PV. and Li, Qi and Zhai, Haijun and Deleger, L. and Deleger, L. and Li, Qi and Lingren, Todd and Champman, WW. and Bridewell, W. and Hanbury, P. and Jing, L. and Huang, H. and Shi, H. and Jones, KS. and Baeza-Yates, RA. and Ribeiro-Neto, BA. and Bland, J. and Altman, D.},
doi = {10.1136/amiajnl-2014-002887},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association JAMIA/Ni et al. - 2015 - Automated clinical trial eligibility prescreening increasing the efficiency of patient identification for clinical tr.pdf:pdf},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association : JAMIA},
keywords = {Automated Clinical Trial Eligibility Screening,Information Extraction,Machine Learning,Natural Language Processing},
number = {1},
pages = {166--78},
pmid = {25030032},
title = {{Automated clinical trial eligibility prescreening: increasing the efficiency of patient identification for clinical trials in the emergency department.}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4433376/pdf/amiajnl-2014-002887.pdf http://www.ncbi.nlm.nih.gov/pubmed/25030032{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4433376},
volume = {22},
year = {2015}
}
@article{DAvolio2010,
abstract = {Reducing custom software development effort is an important goal in information retrieval (IR). This study evaluated a generalizable approach involving with no custom software or rules development. The study used documents "consistent with cancer" to evaluate system performance in the domains of colorectal (CRC), prostate (PC), and lung (LC) cancer. Using an end-user-supplied reference set, the automated retrieval console (ARC) iteratively calculated performance of combinations of natural language processing-derived features and supervised classification algorithms. Training and testing involved 10-fold cross-validation for three sets of 500 documents each. Performance metrics included recall, precision, and F-measure. Annotation time for five physicians was also measured. Top performing algorithms had recall, precision, and F-measure values as follows: for CRC, 0.90, 0.92, and 0.89, respectively; for PC, 0.97, 0.95, and 0.94; and for LC, 0.76, 0.80, and 0.75. In all but one case, conditional random fields outperformed maximum entropy-based classifiers. Algorithms had good performance without custom code or rules development, but performance varied by specific application.},
author = {D'Avolio, Leonard W and Nguyen, Thien M and Farwell, Wildon R and Chen, Yongming and Fitzmeyer, Felicia and Harris, Owen M and Fiore, Louis D},
doi = {10.1136/jamia.2009.001412},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association JAMIA/D'Avolio et al. - 2010 - Evaluation of a generalizable approach to clinical information retrieval using the automated retrieval console.pdf:pdf},
isbn = {1527-974X (Electronic)$\backslash$n1067-5027 (Linking)},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association : JAMIA},
number = {4},
pages = {375--382},
pmid = {20595303},
publisher = {American Medical Informatics Association},
title = {{Evaluation of a generalizable approach to clinical information retrieval using the automated retrieval console (ARC).}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20595303 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2995644},
volume = {17},
year = {2010}
}
@article{Wachter2016,
author = {Wachter, Robert M},
file = {:home/rc538/Documents/References/Unknown/Wachter - 2016 - Making IT work harnessing the power of health IT to improve care in England ( Wachter Review ) Terms of Reference.pdf:pdf},
pages = {71},
title = {{Making IT work : harnessing the power of health IT to improve care in England ( Wachter Review ) Terms of Reference}},
url = {https://www.gov.uk/government/uploads/system/uploads/attachment{\_}data/file/550866/Wachter{\_}Review{\_}Accessible.pdf},
year = {2016}
}
@article{Horvitz2015,
abstract = {Large-scale aggregate analyses of anonymized data can yield valuable results and insights that address public health challenges and provide new avenues for scientific discovery. These methods can extend our knowledge and provide new tools for enhancing health and wellbeing. However, they raise questions about how to best address potential threats to privacy while reaping benefits for individuals and to society as a whole. The use of machine learning to make leaps across informational and social contexts to infer health conditions and risks from nonmedical data provides representative scenarios for reflections on directions with balancing innovation and regulation.},
author = {Horvitz, Eric and Mulligan, Deirdre},
doi = {10.1126/science.aac4520},
file = {:home/rc538/Documents/References/Science/Horvitz, Mulligan - 2015 - Data, privacy, and the greater good.pdf:pdf},
isbn = {0036-8075},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {6245},
pages = {253--255},
pmid = {1000188320},
title = {{Data, privacy, and the greater good}},
url = {http://science.sciencemag.org/content/sci/349/6245/253.full.pdf http://science.sciencemag.org/content/349/6245/253{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26185242},
volume = {349},
year = {2015}
}
@article{Weng2017,
abstract = {Background Current approaches to predict cardiovascular risk fail to identify many people who would benefit from preventive treatment, while others receive unnecessary intervention. Machine- learning offers opportunity to improve accuracy by exploiting complex interactions between risk factors.Weassessed whether machine-learning can improve cardiovascular risk prediction. Methods Prospective cohort study using routine clinical data of 378,256 patients from UK family prac- tices, free from cardiovascular disease at outset. Four machine-learning algorithms (random forest, logistic regression, gradient boosting machines, neural networks) were compared to an established algorithm (American College of Cardiology guidelines) to predict first cardio- vascular event over 10-years. Predictive accuracy was assessed by area under the ‘receiver operating curve' (AUC); and sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) to predict 7.5{\%} cardiovascular risk (threshold for initiating statins). Findings 24,970 incident cardiovascular events (6.6{\%}) occurred. Compared to the established risk prediction algorithm (AUC 0.728, 95{\%} CI 0.723–0.735), machine-learning algorithms improved prediction: random forest +1.7{\%} (AUC 0.745, 95{\%} CI 0.739–0.750), logistic regression +3.2{\%} (AUC 0.760, 95{\%} CI 0.755–0.766), gradient boosting +3.3{\%} (AUC 0.761, 95{\%} CI 0.755–0.766), neural networks +3.6{\%} (AUC 0.764, 95{\%} CI 0.759–0.769). The high- est achieving (neural networks) algorithm predicted 4,998/7,404 cases (sensitivity 67.5{\%}, PPV 18.4{\%}) and 53,458/75,585 non-cases (specificity 70.7{\%}, NPV 95.7{\%}), correctly pre- dicting 355 (+7.6{\%}) more patients who developed cardiovascular disease compared to the established algorithm. Conclusions Machine-learning significantly improves accuracy of cardiovascular risk prediction, increas- ing the number of patients identified who could benefit from preventive treatment, while avoiding unnecessary treatment of others. Introduction},
author = {Weng, Stephen F. and Reps, Jenna and Kai, Joe and Garibaldi, Jonathan M. and Qureshi, Nadeem},
doi = {10.1371/journal.pone.0174944},
editor = {Liu, Bin},
file = {:home/rc538/Documents/References/PLoS ONE/Weng et al. - 2017 - Can machine-learning improve cardiovascular risk prediction using routine clinical data.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {PLoS ONE},
month = {apr},
number = {4},
pages = {1--14},
pmid = {28376093},
title = {{Can machine-learning improve cardiovascular risk prediction using routine clinical data ?}},
url = {http://dx.plos.org/10.1371/journal.pone.0174944},
volume = {355},
year = {2017}
}
@article{Goldstein2016,
abstract = {OBJECTIVE Electronic health records (EHRs) are an increasingly common data source for clinical risk prediction, presenting both unique analytic opportunities and challenges. We sought to evaluate the current state of EHR based risk prediction modeling through a systematic review of clinical prediction studies using EHR data. METHODS We searched PubMed for articles that reported on the use of an EHR to develop a risk prediction model from 2009 to 2014. Articles were extracted by two reviewers, and we abstracted information on study design, use of EHR data, model building, and performance from each publication and supplementary documentation. RESULTS We identified 107 articles from 15 different countries. Studies were generally very large (median sample size = 26 100) and utilized a diverse array of predictors. Most used validation techniques (n = 94 of 107) and reported model coefficients for reproducibility (n = 83). However, studies did not fully leverage the breadth of EHR data, as they uncommonly used longitudinal information (n = 37) and employed relatively few predictor variables (median = 27 variables). Less than half of the studies were multicenter (n = 50) and only 26 performed validation across sites. Many studies did not fully address biases of EHR data such as missing data or loss to follow-up. Average c-statistics for different outcomes were: mortality (0.84), clinical prediction (0.83), hospitalization (0.71), and service utilization (0.71). CONCLUSIONS EHR data present both opportunities and challenges for clinical risk prediction. There is room for improvement in designing such studies.},
author = {Goldstein, Benjamin A and Navar, Ann Marie and Pencina, Michael J and Ioannidis, John PA},
doi = {10.1093/jamia/ocw042},
file = {:home/rc538/Documents/References/Journal of the American Medical Informatics Association JAMIA/Goldstein et al. - 2016 - Opportunities and challenges in developing risk prediction models with electronic health records data a system.pdf:pdf},
isbn = {1067-5027},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association : JAMIA},
keywords = {Electronic Medical Record,Review,Risk Assessment},
pages = {ocw042},
pmid = {27189013},
title = {{Opportunities and challenges in developing risk prediction models with electronic health records data: a systematic review.}},
url = {https://oup.silverchair-cdn.com/oup/backfile/Content{\_}public/Journal/jamia/24/1/10.1093{\_}jamia{\_}ocw042/3/ocw042.pdf?Expires=1493048800{\&}Signature=HbUj8i5C3BBcjCB8SxRw2FV{~}o8v{~}jDqeTar7UpDtzND5GHCH7az1jUCsSbkBZYNZbpMZ8wz-aCrrs4yadoz-mQ3KtsjEADGCBGobrgPtvyrFeTpK7},
volume = {2016},
year = {2016}
}
@article{Walsh2017,
abstract = {Traditional approaches to the prediction of suicide attempts have limited the accuracy and scale of risk detection for these dangerous behaviors. We sought to overcome these limitations by applying machine learning to electronic health records within a large medical database. Participants were 5,167 adult patients with a claim code for self-injury (i.e., ICD-9, E95x); expert review of records determined that 3,250 patients made a suicide attempt (i.e., cases), and 1,917 patients engaged in self-injury that was nonsuicidal, accidental, or nonverifiable (i.e., controls). We developed machine learning algorithms that accurately predicted future suicide attempts (AUC = 0.84, precision = 0.79, recall = 0.95, Brier score = 0.14). Moreover, accuracy improved from 720 days to 7 days before the suicide attempt, and predictor importance shifted across time. These findings represent a step toward accurate and scalable risk detection and provide insight into how suicide attempt risk shifts over time.},
author = {Walsh, Colin G. and Ribeiro, Jessica D. and Franklin, Joseph C.},
doi = {10.1177/2167702617691560},
file = {:home/rc538/Documents/References/Clinical Psychological Science/Walsh, Ribeiro, Franklin - 2017 - Predicting Risk of Suicide Attempts over Time through Machine Learning.pdf:pdf},
issn = {2167-7026},
journal = {Clinical Psychological Science},
keywords = {classification,prediction,prevention,suicide prevention},
month = {apr},
pages = {1--13},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Predicting Risk of Suicide Attempts over Time through Machine Learning}},
url = {http://journals.sagepub.com/doi/10.1177/2167702617691560},
year = {2017}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy 1–3 , is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) 4,5 show potential for general and highly variable tasks across many fine-grained object categories 6–11 . Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets 12 —consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care. There are 5.4 million new cases of skin cancer in the United States 2 every year. One in five Americans will be diagnosed with a cutaneous malignancy in their lifetime. Although melanomas represent fewer than 5{\%} of all skin cancers in the United States, they account for approxi-mately 75{\%} of all skin-cancer-related deaths, and are responsible for over 10,000 deaths annually in the United States alone. Early detection is critical, as the estimated 5-year survival rate for melanoma drops from over 99{\%} if detected in its earliest stages to about 14{\%} if detected in its latest stages. We developed a computational method which may allow medical practitioners and patients to proactively track skin lesions and detect cancer earlier. By creating a novel disease taxonomy, and a disease-partitioning algorithm that maps individual diseases into training classes, we are able to build a deep learning system for auto-mated dermatology. Previous work in dermatological computer-aided classification 12,14,15},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
doi = {10.1038/nature21056},
file = {:home/rc538/Documents/References/Nature/Esteva et al. - 2017 - Dermatologist-level classification of skin cancer with deep neural networks.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
pmid = {28117445},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
volume = {542},
year = {2017}
}
